王伟超   wangweichao@tedu.cn   备注：班级-姓名-代码

1、第四阶段课程介绍 - 总时长：13个工作日
   1.1》爬虫(7天)
	      爬虫工程师
				其他岗位附加技能（后端开发、数据分析）
   1.2》Hadoop(3天)
	      数据分析岗位的附加技能
   1.3》数据结构(3天)
	      所有技术岗位的附加技能
2、第四阶段课程特点
   2.1》爬虫：综合、不确定性
	      第一阶段、MySQL、MongoDB、Redis、re、多线程、
				多进程、进程锁、队列、HTML、JavaScript......
	 2.2》Hadoop：抽象
	 2.3》数据结构：抽象
3、爬虫分类
   3.1》通用网络爬虫 - 搜索引擎,需要遵守robots协议
	      ROBOTS协议：网站通过ROBOTS协议告诉搜索引擎哪些
				页面可以抓取,哪些页面不能抓(君子协议)
	 3.2》聚焦网络爬虫
4、请求模块-requests
   4.1》resp = requests.get(url='',headers={})
	 4.2》响应对象属性
	      4.2.1》text : 响应内容 - 字符串
				4.2.2》content : 响应内容 - bytes
				4.2.3》status_code : HTTP响应码
				4.2.4》url : 返回实际数据的URL地址
5、编码模块-urllib.parse
   5.1》urlencode({})
	 5.2》quote('')
	 5.3》unquote('')
6、解析模块-re
   6.1》使用流程：r_list = re.findall(regex, html, re.S)
	 6.2》正则分组：需要抓取什么数据就加(.*?)
	      情况1：[] 要么是正则问题,要么是响应问题
				情况2：['', '', ...] 正则中一个分组
				情况3：[(), (), ...] 正则中多个分组
7、数据抓取流程
   7.1》确认数据来源 - 右键查看网页源代码,搜索关键字
	 7.2》存在:观察URL地址的规律
	 7.3》写正则表达式
	 7.4》写程序
	      细节1：必须使用User-Agent
				细节2：必须控制数据抓取的频率
8、数据持久化代码流程 - MySQL
   8.1》__init__() 创建数据库连接对象和游标对象
	 8.2》数据处理函数中,将数据处理为列表,存入数据库
	 8.3》所有页的数据抓取完成后,断开数据库的连接(crawl())
9、数据持久化代码流程 - csv
   9.1》__init__() 打开文件,创建csv文件写入对象
	 9.2》数据处理器函数中,将数据处理为列表,写入csv文件
	 9.3》所有页的数据抓取完成后,关闭文件(crawl())
10、数据持久化代码流程 - MongoDB
   10.1》__init__() 创建连接对象、库对象、集合对象
	 10.2》数据处理函数中,将数据处理为字典,存入数据库
11、两级或者多级页面数据抓取流程
   11.1》需要定义功能函数(请求、解析)
	 11.2》主线函数:一级页面解析函数
	       一旦提取出需要继续跟进的URL地址,则写个函数
				 把所有的事情都做了,然后再遍历下一个URL地址
12、Chrome浏览器安装插件
   1、安装方法
	    1.1》浏览器找到更多工具-扩展程序-点开开发者模式
			1.2》将解压后的谷歌访问助手拖拽到浏览器中
			1.3》通过谷歌访问助手进入到Chrome应用商店在线安装
   2、爬虫常用插件
	    2.1》Xpath Helper
			     开启|关闭：Ctrl + Shift + x
			2.2》JsonView：格式化输出JSON数据









